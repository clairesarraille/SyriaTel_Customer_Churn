{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "- Why are you using machine learning rather than a simpler approach?\n",
    "- What is it about the problem/data that is suitable for logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- One-hot-encoding here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "- Pattern of error?\n",
    "- Consider the business problem when choosing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIVE! Eyes on the Prize!\n",
    "- Predictive Findings:\n",
    "  - How well your model is able to predict target\n",
    "  - Which features are most important to model\n",
    "- Predictive Recommendations:\n",
    "  - Context and situation where predictions would be useful\n",
    "  - Suggest to data engineers how data can be transformed upon ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CRISP-DM\n",
    "- Dummy model\n",
    "- Evaluate on appropriate classification metrics (bias/variance?)\n",
    "- Proceed to next model (provide justification!)\n",
    "- Create a classifier that beats dummy model - doesn't have to be perfect\n",
    "  - When your model isn't improving and you've tuned a couple, you can stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Viable Product:\n",
    "- Fit the transformer on the training data and use it to transform both the train and test\n",
    "- Never fit anything to test\n",
    "- Don't use target as a feature or a numeric target\n",
    "- Don't use regularization on a model that is overfitting\n",
    "- Report the model's performance on the TEST data, not the training data.\n",
    "- Use at least 2 types of scikit-learn models (logistic regression and ridge-lasso good enough?)\n",
    "- Tune at least 1 hyperparameter in a justifiable way without any major errors\n",
    "- Focus on specific metrics that are important to business case (not just displaying `classification_report` and/or confusion matrix -- you wouldn't want to try and discuss ALL evaluation metrics, and you also wouldn't want to just display the metrics without discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "- Evaluate based on TEST data\n",
    "- Bias/Variance - overfitting vs. underfitting\n",
    "- Recall\n",
    "- Precision\n",
    "- Accuracy\n",
    "- MSE (better than R-squared for explaining to stakeholders)\n",
    "- Confusion Matrix\n",
    "- `classification_report`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune! Make justifiable changes in successive models:\n",
    "    - DO NOT reduce regularization on a model that is overfitting\n",
    "    - Threshold: Calculate from ROC Curve:\n",
    "      - https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    - Tune Hyperparameters and Grid Search: \n",
    "      - https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "    - Manipulate target?\n",
    "    - Maximize validation scores\n",
    "      - As a validation score, accuracy is inappropriate for imbalanced classification problems\n",
    "        - Use precision, recall, F-Measure for imbalanced classification: https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "      - Cross validation: https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/\n",
    "    - Consider Bias/Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "- After refining models, provide 1-3 paragraphs discussing final model and at least 1 overall model metric\n",
    "- Model Limitations Discussion:\n",
    "  - Records/Instances where model performance was worse (Question: Does this mean an individual row?)\n",
    "  - If used in production, what kinds of problems would pop up?\n",
    "  - Connect metrics to real-world implications \n",
    "    - What should stakeholders do with this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional:\n",
    "- Cross Validation\n",
    "- Feature Engineering\n",
    "- Pipelines\n",
    "  - Only use pipelines later if you have time\n",
    "  - Pipelines help prevent data leakage\n",
    "  - They transform the test set exactly how train was transformed\n",
    "  - Pipelines are the best-practice approach to data preparation that avoids leakage, but they can get complicated very quickly. We therefore do not recommend that you use pipelines in your initial modeling approach, but rather that you refactor to use pipelines if you have time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for later:\n",
    "#### International Plan Feature:\n",
    "- How could we tease apart \"international plan\" and \"total intl calls\"/minutes in order to see how likely a customer with a high number of international minutes but no international plan is likely to churn?\n",
    "- See if people with no international plan and high international minutes/calls is more or less likely to churn\n",
    "- Is no intl. plan correlated + high minutes correlated to churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts on Data (Consider the business problem when choosing features)\n",
    "- Area codes (and by associate phone numbers) and State do not match (415 is not an area code in Kansas)\n",
    "- \"State\" may be a useful geographical feature to consider, but lots of people live in states that don't match their phone #'s area code, so area code isn't a reliable indicator of location.\n",
    "- There are no nulls\n",
    "- Categorical Variables (besides target which is Churn)\n",
    "- ## Numeric vs. Categorical:\n",
    "  - Is it numeric or categorical?\n",
    "    - As \"Is an increase of 2 in this variable twice as much as an increase of 1?\"\n",
    "  - State\n",
    "- These are boolean value columns - so they don't need to be one-hot-encoded, just converted from yes/no to 1/0\n",
    "  - international plan\n",
    "  - voice mail plan\n",
    "- Ordinal values -- there are none\n",
    "- To Drop:\n",
    "  -   Area Code (because an increase of 1 does'nt mean twice as many)\n",
    "  -   Phone number (because an increase of 1 does'nt mean twice as many)\n",
    "- Calls vs. Minutes\n",
    "  - The more calls doesn't necessarily mean more minutes, so we will keep calls and minutes (they are not redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "- Pattern of error?\n",
    "\n",
    "- Cannabilize the violin plot function in here for EDA:\n",
    "  - 41-classification_workflow-completed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose most important features by finding ones highest correlated with target:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIVE! Eyes on the Prize!\n",
    "- Predictive Findings:\n",
    "  - How well your model is able to predict target\n",
    "  - Which features are most important to model\n",
    "- Predictive Recommendations:\n",
    "  - Context and situation where predictions would be useful\n",
    "  - Suggest to data engineers how data can be transformed upon ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Iterative Modeling Process:\n",
    "1. Dummy model\n",
    "2. Evaluate on appropriate classification metrics (bias/variance?)\n",
    "3. Proceed to next model (provide justification!)\n",
    "- Ultimate Goal is to Create a classifier that beats dummy model - doesn't have to be perfect\n",
    "  - When your model isn't improving and you've tuned a couple, you can stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Modeling Process us Cross-Validation:\n",
    "### Process below is from this lecture (also has lots on how to appropriately use Regularization): ***36-Regularization_Lecture-completed.ipynb***\n",
    "\n",
    "So, now our modeling process has an added step: cross-validation.\n",
    "\n",
    "1. Get Data\n",
    "2. EDA\n",
    "3. Cleaning\n",
    "4. Feature Engineering\n",
    "5. Train/Test split\n",
    "6. Model training using `train` split\n",
    "7. Cross Validation (Once you are happy with the model, then do step 8)\n",
    "8. Model testing using `test` split\n",
    "\n",
    "Please note, this is **NOT** a linear process.\n",
    "\n",
    "You will repeat steps 3 through 7 many times. \n",
    "\n",
    "You only use the `test` split when you are satisfied of your model's performance as judged by the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See this lecture for example of entire logistic regression (classification) workflow:\n",
    "### **41-classification_workflow-completed.ipynb**\n",
    "- Adapt the \"\"ModelwithCV\" class\" thingie - an copy it but just cite source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Viable Product:\n",
    "- Use logistic regression, no need for any other algorithm\n",
    "- Fit the transformer on the training data and use it to transform both the train and test\n",
    "- Never fit anything to test\n",
    "- Don't use target as a feature or a numeric target\n",
    "- Don't REDUCE regularization on a model that is overfitting. INCREASE regularization if model is overfitting.\n",
    "- Report the model's performance on the TEST data, not the training data.\n",
    "- Tune at least 1 hyperparameter in a justifiable way without any major errors\n",
    "- Focus on specific metrics that are important to business case (not just displaying `classification_report` and/or confusion matrix -- you wouldn't want to try and discuss ALL evaluation metrics, and you also wouldn't want to just display the metrics without discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model & Initial feature selection:\n",
    "1. Choosing features is indeed an iterative process. For the dummy model in logistic regression, it's simply predicting the majority class. For logistic regression, which is a type of linear model, you can use all columns to start out with or take the top 3-5 columns most correlated with the target. For subsequent iterative models, can choose different cols. If model is underfit, can increase complexity (variance) with polynomial features. If overfit, good approach is increasing regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "- Evaluate based on TEST data\n",
    "- Bias/Variance - overfitting vs. underfitting\n",
    "- Recall\n",
    "- Precision\n",
    "- Accuracy\n",
    "- MSE (better than R-squared for explaining to stakeholders)\n",
    "- Confusion Matrix\n",
    "- `classification_report`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning! Make justifiable changes in successive models:\n",
    "- DO NOT reduce regularization on a model that is overfitting.\n",
    "  - Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and wonâ€™t overfit.\n",
    "- Threshold: Calculate from ROC Curve:\n",
    "  - https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "- Tune Hyperparameters and Grid Search:\n",
    "  - https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "- Manipulate target?\n",
    "- Maximize validation scores\n",
    "  - As a validation score, accuracy is inappropriate for imbalanced classification problems\n",
    "  - Use precision, recall, F-Measure for imbalanced classification: https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "- Cross validation: https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/\n",
    "- Consider Bias/Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "- After refining models, provide 1-3 paragraphs discussing final model and at least 1 overall model metric\n",
    "- Model Limitations Discussion:\n",
    "  - Records/Instances where model performance was worse (Question: Does this mean an individual row?)\n",
    "  - If used in production, what kinds of problems would pop up?\n",
    "  - Connect metrics to real-world implications \n",
    "    - What should stakeholders do with this information?\n",
    "\n",
    "This can include rows/individuals in training set that are outliers - maybe your model performs especially bad on those. Basically, the \"limitations\" part of your final discussion should address weaknesses in data and algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional:\n",
    "- Cross Validation\n",
    "- Feature Engineering\n",
    "- Pipelines\n",
    "  - Only use pipelines later if you have time\n",
    "  - Pipelines help prevent data leakage\n",
    "  - They transform the test set exactly how train was transformed\n",
    "  - Pipelines are the best-practice approach to data preparation that avoids leakage, but they can get complicated very quickly. We therefore do not recommend that you use pipelines in your initial modeling approach, but rather that you refactor to use pipelines if you have time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
